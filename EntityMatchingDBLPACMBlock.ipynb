{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matching\n",
    "\n",
    "The process of finding matching records in tables is called entity matching. It can be used for deduplication of tables or to link tables to merge the information that they contain. The question that we want to answer in this notebook is whether machine learning has something to add to traditional programming methods for entity matching and whether fancy deep learning approaches really outperform simpler machine learning moedels that have already been around for decades.\n",
    "\n",
    "To investigate this, we make use of two existing entity matching packages: the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) (PRLT), which offers support for traditional programmimng techniques and basic machine learning models and [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/), a package that adopts deep learning techniques. We test both packages on  scientific bibliographic data from [ACM](https://www.acm.org/) and [DBLP](https://dblp.org/), which we downloaded from the [Database Group Leipzig ](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution). This is an independent dataset that was not used to advertise either of the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports\n",
    "\n",
    "We now import the packages that we are going to use, together with our self-created module em_helper that contains some utitility functions to generate and display clasification performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepmatcher as dm\n",
    "from em_helper import print_results, performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Having all our dependencies in place, we now load and display the two tables that contain the records that we want to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>journals/sigmod/Mackay99</th>\n",
       "      <td>Semantic Integration of Environmental Models f...</td>\n",
       "      <td>D. Scott Mackay</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/PoosalaI96</th>\n",
       "      <td>Estimation of Query-Result Distribution and it...</td>\n",
       "      <td>Viswanath Poosala, Yannis E. Ioannidis</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/PalpanasSCP02</th>\n",
       "      <td>Incremental Maintenance for Non-Distributive A...</td>\n",
       "      <td>Themistoklis Palpanas, Richard Sidle, Hamid Pi...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/GardarinGT96</th>\n",
       "      <td>Cost-based Selection of Path Expression Proces...</td>\n",
       "      <td>Zhao-Hui Tang, Georges Gardarin, Jean-Robert G...</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/HoelS95</th>\n",
       "      <td>Benchmarking Spatial Join Operations with Spat...</td>\n",
       "      <td>Erik G. Hoel, Hanan Samet</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journals/tods/KarpSP03</th>\n",
       "      <td>A simple algorithm for finding frequent elemen...</td>\n",
       "      <td>Scott Shenker, Christos H. Papadimitriou, Rich...</td>\n",
       "      <td>ACM Trans. Database Syst.</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/LimWV03</th>\n",
       "      <td>SASH: A Self-Adaptive Histogram Set for Dynami...</td>\n",
       "      <td>Lipyeow Lim, Min Wang, Jeffrey Scott Vitter</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journals/tods/ChakrabartiKMP02</th>\n",
       "      <td>Locally adaptive dimensionality reduction for ...</td>\n",
       "      <td>Kaushik Chakrabarti, Eamonn J. Keogh, Michael ...</td>\n",
       "      <td>ACM Trans. Database Syst.</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journals/sigmod/Snodgrass01</th>\n",
       "      <td>Chair's Message</td>\n",
       "      <td>Richard T. Snodgrass</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conf/vldb/LiM01</th>\n",
       "      <td>Indexing and Querying XML Data for Regular Pat...</td>\n",
       "      <td>Bongki Moon, Quanzhong Li</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2616 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            title  \\\n",
       "id                                                                                  \n",
       "journals/sigmod/Mackay99        Semantic Integration of Environmental Models f...   \n",
       "conf/vldb/PoosalaI96            Estimation of Query-Result Distribution and it...   \n",
       "conf/vldb/PalpanasSCP02         Incremental Maintenance for Non-Distributive A...   \n",
       "conf/vldb/GardarinGT96          Cost-based Selection of Path Expression Proces...   \n",
       "conf/vldb/HoelS95               Benchmarking Spatial Join Operations with Spat...   \n",
       "...                                                                           ...   \n",
       "journals/tods/KarpSP03          A simple algorithm for finding frequent elemen...   \n",
       "conf/vldb/LimWV03               SASH: A Self-Adaptive Histogram Set for Dynami...   \n",
       "journals/tods/ChakrabartiKMP02  Locally adaptive dimensionality reduction for ...   \n",
       "journals/sigmod/Snodgrass01                                       Chair's Message   \n",
       "conf/vldb/LiM01                 Indexing and Querying XML Data for Regular Pat...   \n",
       "\n",
       "                                                                          authors  \\\n",
       "id                                                                                  \n",
       "journals/sigmod/Mackay99                                          D. Scott Mackay   \n",
       "conf/vldb/PoosalaI96                       Viswanath Poosala, Yannis E. Ioannidis   \n",
       "conf/vldb/PalpanasSCP02         Themistoklis Palpanas, Richard Sidle, Hamid Pi...   \n",
       "conf/vldb/GardarinGT96          Zhao-Hui Tang, Georges Gardarin, Jean-Robert G...   \n",
       "conf/vldb/HoelS95                                       Erik G. Hoel, Hanan Samet   \n",
       "...                                                                           ...   \n",
       "journals/tods/KarpSP03          Scott Shenker, Christos H. Papadimitriou, Rich...   \n",
       "conf/vldb/LimWV03                     Lipyeow Lim, Min Wang, Jeffrey Scott Vitter   \n",
       "journals/tods/ChakrabartiKMP02  Kaushik Chakrabarti, Eamonn J. Keogh, Michael ...   \n",
       "journals/sigmod/Snodgrass01                                  Richard T. Snodgrass   \n",
       "conf/vldb/LiM01                                         Bongki Moon, Quanzhong Li   \n",
       "\n",
       "                                                    venue  year  \n",
       "id                                                               \n",
       "journals/sigmod/Mackay99                    SIGMOD Record  1999  \n",
       "conf/vldb/PoosalaI96                                 VLDB  1996  \n",
       "conf/vldb/PalpanasSCP02                              VLDB  2002  \n",
       "conf/vldb/GardarinGT96                               VLDB  1996  \n",
       "conf/vldb/HoelS95                                    VLDB  1995  \n",
       "...                                                   ...   ...  \n",
       "journals/tods/KarpSP03          ACM Trans. Database Syst.  2003  \n",
       "conf/vldb/LimWV03                                    VLDB  2003  \n",
       "journals/tods/ChakrabartiKMP02  ACM Trans. Database Syst.  2002  \n",
       "journals/sigmod/Snodgrass01                 SIGMOD Record  2001  \n",
       "conf/vldb/LiM01                                      VLDB  2001  \n",
       "\n",
       "[2616 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304586</th>\n",
       "      <td>The WASA2 object-oriented workflow management ...</td>\n",
       "      <td>Gottfried Vossen, Mathias Weske</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304587</th>\n",
       "      <td>A user-centered interface for querying distrib...</td>\n",
       "      <td>Isabel F. Cruz, Kimberly M. James</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304589</th>\n",
       "      <td>World Wide Database-integrating the Web, CORBA...</td>\n",
       "      <td>Athman Bouguettaya, Boualem Benatallah, Lily H...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304590</th>\n",
       "      <td>XML-based information mediation with MIX</td>\n",
       "      <td>Chaitan Baru, Amarnath Gupta, Bertram Lud&amp;#228...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304582</th>\n",
       "      <td>The CCUBE constraint object-oriented database ...</td>\n",
       "      <td>Alexander Brodsky, Victor E. Segal, Jia Chen, ...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672977</th>\n",
       "      <td>Dual-Buffering Strategies in Object Bases</td>\n",
       "      <td>Alfons Kemper, Donald Kossmann</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950482</th>\n",
       "      <td>Guest editorial</td>\n",
       "      <td>Philip A. Bernstein, Yannis Ioannidis, Raghu R...</td>\n",
       "      <td>The VLDB Journal &amp;mdash; The International Jou...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672980</th>\n",
       "      <td>GraphDB: Modeling and Querying Graphs in Datab...</td>\n",
       "      <td>Ralf Hartmut G&amp;#252;ting</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945741</th>\n",
       "      <td>Review of The data warehouse toolkit: the comp...</td>\n",
       "      <td>Alexander A. Anisimov</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672979</th>\n",
       "      <td>Bulk Loading into an OODB: A Performance Study</td>\n",
       "      <td>Janet L. Wiener, Jeffrey F. Naughton</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2294 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "id                                                          \n",
       "304586  The WASA2 object-oriented workflow management ...   \n",
       "304587  A user-centered interface for querying distrib...   \n",
       "304589  World Wide Database-integrating the Web, CORBA...   \n",
       "304590           XML-based information mediation with MIX   \n",
       "304582  The CCUBE constraint object-oriented database ...   \n",
       "...                                                   ...   \n",
       "672977          Dual-Buffering Strategies in Object Bases   \n",
       "950482                                    Guest editorial   \n",
       "672980  GraphDB: Modeling and Querying Graphs in Datab...   \n",
       "945741  Review of The data warehouse toolkit: the comp...   \n",
       "672979     Bulk Loading into an OODB: A Performance Study   \n",
       "\n",
       "                                                  authors  \\\n",
       "id                                                          \n",
       "304586                    Gottfried Vossen, Mathias Weske   \n",
       "304587                  Isabel F. Cruz, Kimberly M. James   \n",
       "304589  Athman Bouguettaya, Boualem Benatallah, Lily H...   \n",
       "304590  Chaitan Baru, Amarnath Gupta, Bertram Lud&#228...   \n",
       "304582  Alexander Brodsky, Victor E. Segal, Jia Chen, ...   \n",
       "...                                                   ...   \n",
       "672977                     Alfons Kemper, Donald Kossmann   \n",
       "950482  Philip A. Bernstein, Yannis Ioannidis, Raghu R...   \n",
       "672980                           Ralf Hartmut G&#252;ting   \n",
       "945741                              Alexander A. Anisimov   \n",
       "672979               Janet L. Wiener, Jeffrey F. Naughton   \n",
       "\n",
       "                                                    venue  year  \n",
       "id                                                               \n",
       "304586     International Conference on Management of Data  1999  \n",
       "304587     International Conference on Management of Data  1999  \n",
       "304589     International Conference on Management of Data  1999  \n",
       "304590     International Conference on Management of Data  1999  \n",
       "304582     International Conference on Management of Data  1999  \n",
       "...                                                   ...   ...  \n",
       "672977                              Very Large Data Bases  1994  \n",
       "950482  The VLDB Journal &mdash; The International Jou...  2003  \n",
       "672980                              Very Large Data Bases  1994  \n",
       "945741                                 ACM SIGMOD Record   2003  \n",
       "672979                              Very Large Data Bases  1994  \n",
       "\n",
       "[2294 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_df = pd.read_csv('Data/DBLP2.csv', encoding='cp1252')\n",
    "right_df = pd.read_csv('Data/ACM.csv', encoding='cp1252')\n",
    "left_df.set_index('id',inplace=True)\n",
    "right_df.set_index('id',inplace=True)\n",
    "display(left_df)\n",
    "display(right_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Candidate Links\n",
    "To translate the matching problem into a standard classification problem, we first form all possible combinations of rows in the DBLP table and rows in the ACM table. In other words, we form the Cartesian product of the indices of these tables. The DBLP table has 2616 rows and the the ACM table has 2294 rows, so this results in a collection of 2616 x 2294 = 6001104 combinations. \n",
    "\n",
    "The size of this set of possible links now places us for a dilemma:\n",
    "1. We would like to use as much of the data as possible to reach a sound conclusion\n",
    "2. DeepMatcher is not that fast and will take ages to process such a  large dataset.\n",
    "\n",
    "To solve this dilemma, we apply a technique that is commonly used in entity matching: _blocking_. By using a simple heuristic, we filter out row combinations, of which we can be (almost) certain that they do not match. By blocking these row combinations, we end up with a considerably smaller set of candidate links that is much easier to manage. \n",
    "\n",
    "To perform blocking, we make use of the string comparison capabilities of the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/). We block all combinations with a [Lehvenstein-distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)-based similarity measure that is smaller than 0.3, where 0 indicates a complete mismatch and 1 indicates identical strings. By doing this, we reduce the number of candidate links to 162894, which amounts to a data reduction of about 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([('journals/sigmod/Mackay99', 304586),\n",
       "            ('journals/sigmod/Mackay99', 304587),\n",
       "            ('journals/sigmod/Mackay99', 304589),\n",
       "            ('journals/sigmod/Mackay99', 304590),\n",
       "            ('journals/sigmod/Mackay99', 304582),\n",
       "            ('journals/sigmod/Mackay99', 304583),\n",
       "            ('journals/sigmod/Mackay99', 304584),\n",
       "            ('journals/sigmod/Mackay99', 304585),\n",
       "            ('journals/sigmod/Mackay99', 306112),\n",
       "            ('journals/sigmod/Mackay99', 306115),\n",
       "            ...\n",
       "            (         'conf/vldb/LiM01', 945735),\n",
       "            (         'conf/vldb/LiM01', 672983),\n",
       "            (         'conf/vldb/LiM01', 950484),\n",
       "            (         'conf/vldb/LiM01', 672978),\n",
       "            (         'conf/vldb/LiM01', 950483),\n",
       "            (         'conf/vldb/LiM01', 672977),\n",
       "            (         'conf/vldb/LiM01', 950482),\n",
       "            (         'conf/vldb/LiM01', 672980),\n",
       "            (         'conf/vldb/LiM01', 945741),\n",
       "            (         'conf/vldb/LiM01', 672979)],\n",
       "           names=['id_1', 'id_2'], length=6001104)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MultiIndex([('journals/sigmod/Mackay99', 304570),\n",
       "            ('journals/sigmod/Mackay99', 309852),\n",
       "            ('journals/sigmod/Mackay99', 309849),\n",
       "            ('journals/sigmod/Mackay99', 309895),\n",
       "            ('journals/sigmod/Mackay99', 310061),\n",
       "            ('journals/sigmod/Mackay99', 309897),\n",
       "            ('journals/sigmod/Mackay99', 276345),\n",
       "            ('journals/sigmod/Mackay99', 304183),\n",
       "            ('journals/sigmod/Mackay99', 375733),\n",
       "            ('journals/sigmod/Mackay99', 375769),\n",
       "            ...\n",
       "            (         'conf/vldb/LiM01', 872796),\n",
       "            (         'conf/vldb/LiM01', 872809),\n",
       "            (         'conf/vldb/LiM01', 872815),\n",
       "            (         'conf/vldb/LiM01', 767135),\n",
       "            (         'conf/vldb/LiM01', 603868),\n",
       "            (         'conf/vldb/LiM01', 958947),\n",
       "            (         'conf/vldb/LiM01', 672818),\n",
       "            (         'conf/vldb/LiM01', 959076),\n",
       "            (         'conf/vldb/LiM01', 672827),\n",
       "            (         'conf/vldb/LiM01', 950484)],\n",
       "           names=['id_1', 'id_2'], length=162894)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Full link\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.full()\n",
    "links = indexer.index(left_df, right_df)\n",
    "display(links)\n",
    "\n",
    "# Comparison step for blocking\n",
    "compare_cl = recordlinkage.Compare()\n",
    "compare_cl.string('title', 'title', threshold=0.3, label='title')\n",
    "features = compare_cl.compute(links, left_df, right_df)\n",
    "\n",
    "# Blocking\n",
    "candidate_links = links[features.sum(axis=1) > 0]\n",
    "display(candidate_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of blocking effects\n",
    "Since blocking might have the unwanted effect of throwing away a considerable number of true mtaches, we briefly verify whether this is the case. We load the data set containing the true links and see how much of these links are still present in the blocked data set. It turns out that only 8 of the 2224 true matches are discarded by the blocking procesdure, which amounts to 0.36%. This seems to be a defendable sacrifice to make for the 97% data reduction that is achieved by blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matches = pd.read_csv('Data/DBLP-ACM_perfectMapping.csv', encoding='cp1252')\n",
    "matches_tuples = list(matches.itertuples(index=False, name=None)) \n",
    "candidate_links_tuples = candidate_links.tolist()\n",
    "intersection = set(candidate_links_tuples).intersection(maches_tuples)\n",
    "display(len(intersection))\n",
    "display(len(matches_tuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for DeepMatcher\n",
    "To use the DeepMatcher classifier, we need to transform our data into something that [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) can understand. DeepMatcher takes in a table with record pairs of the two tables, which need to be matched. The records of the first table  are marked by adding the prefix \"left_\" to their feature names, and the records of the second table are marked by adding the prefix \"right_\". The target variable that indicates whether a pair forms a match is called \"label\" and takes on the vlaues 0 (no match) or 1 (match). To prevent unwanted correlations between the subsequent rows, we additionally shuffle the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>left_title</th>\n",
       "      <th>left_authors</th>\n",
       "      <th>left_venue</th>\n",
       "      <th>left_year</th>\n",
       "      <th>right_title</th>\n",
       "      <th>right_authors</th>\n",
       "      <th>right_venue</th>\n",
       "      <th>right_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4003186</th>\n",
       "      <td>0</td>\n",
       "      <td>Semistructured und Structured Data in the Web:...</td>\n",
       "      <td>Paolo Merialdo, Giansalvatore Mecca, Paolo Atzeni</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1997</td>\n",
       "      <td>Olympic records for data at the 1998 Nagano games</td>\n",
       "      <td>Edwin R. Lassettre</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485817</th>\n",
       "      <td>0</td>\n",
       "      <td>An Automated System for Web Portal Personaliza...</td>\n",
       "      <td>Philip S. Yu, Charu C. Aggarwal</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2002</td>\n",
       "      <td>Automatic subspace clustering of high dimensio...</td>\n",
       "      <td>Rakesh Agrawal, Johannes Gehrke, Dimitrios Gun...</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581657</th>\n",
       "      <td>0</td>\n",
       "      <td>Declustering of Key-Based Partitioned Signatur...</td>\n",
       "      <td>Pavel Zezula, Paolo Ciaccia, Paolo Tiberio</td>\n",
       "      <td>ACM Trans. Database Syst.</td>\n",
       "      <td>1996</td>\n",
       "      <td>Distinguished database profiles</td>\n",
       "      <td>Marianne Winslett</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835435</th>\n",
       "      <td>0</td>\n",
       "      <td>The Jungle Database Search Engine</td>\n",
       "      <td>Linas Bukauskas, Curtis E. Dyreson, Michael H....</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1999</td>\n",
       "      <td>Very Large Databases: How Large, How Different?</td>\n",
       "      <td>David Vaskevitch</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575416</th>\n",
       "      <td>0</td>\n",
       "      <td>Information integration on the Web: a view fro...</td>\n",
       "      <td>Subbarao Kambhampati, Craig A. Knoblock</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>2003</td>\n",
       "      <td>Data extraction and transformation for the dat...</td>\n",
       "      <td>Case Squire</td>\n",
       "      <td>International Conference on Management of Data</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959276</th>\n",
       "      <td>0</td>\n",
       "      <td>An Open Storage System for Abstract Objects</td>\n",
       "      <td>Lukas Relly, Stephen Blott, Hans-JÃ¶rg Schek</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1996</td>\n",
       "      <td>Lore: a database management system for semistr...</td>\n",
       "      <td>Jason McHugh, Serge Abiteboul, Roy Goldman, Da...</td>\n",
       "      <td>ACM SIGMOD Record</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5809458</th>\n",
       "      <td>0</td>\n",
       "      <td>Optimizing Queries on Compressed Bitmaps</td>\n",
       "      <td>Sihem Amer-Yahia, Theodore Johnson</td>\n",
       "      <td>VLDB</td>\n",
       "      <td>2000</td>\n",
       "      <td>Reordering Query Execution in Tertiary Memory ...</td>\n",
       "      <td>Sunita Sarawagi, Michael Stonebraker</td>\n",
       "      <td>Very Large Data Bases</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250584</th>\n",
       "      <td>0</td>\n",
       "      <td>ARIADNE: A System for Constructing Mediators f...</td>\n",
       "      <td>Ion Muslea, Greg Barish, Andrew Philpot, Craig...</td>\n",
       "      <td>SIGMOD Conference</td>\n",
       "      <td>1998</td>\n",
       "      <td>Cost models for overlapping and multiversion s...</td>\n",
       "      <td>Yufei Tao, Dimitris Papadias, Jun Zhang</td>\n",
       "      <td>ACM Transactions on Database Systems (TODS)</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889105</th>\n",
       "      <td>0</td>\n",
       "      <td>The TriGS Active Object-Oriented Database Syst...</td>\n",
       "      <td>Gerti Kappel, Werner Retschitzegger</td>\n",
       "      <td>SIGMOD Record</td>\n",
       "      <td>1998</td>\n",
       "      <td>Index configuration in object-oriented databases</td>\n",
       "      <td>Elisa Bertino</td>\n",
       "      <td>The VLDB Journal &amp;mdash; The International Jou...</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441258</th>\n",
       "      <td>0</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>Richard T. Snodgrass</td>\n",
       "      <td>ACM Trans. Database Syst.</td>\n",
       "      <td>2001</td>\n",
       "      <td>Guest editorial</td>\n",
       "      <td>Vijay Atluri, Anupam Joshi, Yelena Yesha</td>\n",
       "      <td>The VLDB Journal &amp;mdash; The International Jou...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162894 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                         left_title  \\\n",
       "id                                                                  \n",
       "4003186      0  Semistructured und Structured Data in the Web:...   \n",
       "485817       0  An Automated System for Web Portal Personaliza...   \n",
       "2581657      0  Declustering of Key-Based Partitioned Signatur...   \n",
       "3835435      0                  The Jungle Database Search Engine   \n",
       "2575416      0  Information integration on the Web: a view fro...   \n",
       "...        ...                                                ...   \n",
       "5959276      0        An Open Storage System for Abstract Objects   \n",
       "5809458      0           Optimizing Queries on Compressed Bitmaps   \n",
       "250584       0  ARIADNE: A System for Constructing Mediators f...   \n",
       "4889105      0  The TriGS Active Object-Oriented Database Syst...   \n",
       "5441258      0                                          Editorial   \n",
       "\n",
       "                                              left_authors  \\\n",
       "id                                                           \n",
       "4003186  Paolo Merialdo, Giansalvatore Mecca, Paolo Atzeni   \n",
       "485817                     Philip S. Yu, Charu C. Aggarwal   \n",
       "2581657         Pavel Zezula, Paolo Ciaccia, Paolo Tiberio   \n",
       "3835435  Linas Bukauskas, Curtis E. Dyreson, Michael H....   \n",
       "2575416            Subbarao Kambhampati, Craig A. Knoblock   \n",
       "...                                                    ...   \n",
       "5959276        Lukas Relly, Stephen Blott, Hans-JÃ¶rg Schek   \n",
       "5809458                 Sihem Amer-Yahia, Theodore Johnson   \n",
       "250584   Ion Muslea, Greg Barish, Andrew Philpot, Craig...   \n",
       "4889105                Gerti Kappel, Werner Retschitzegger   \n",
       "5441258                               Richard T. Snodgrass   \n",
       "\n",
       "                        left_venue  left_year  \\\n",
       "id                                              \n",
       "4003186              SIGMOD Record       1997   \n",
       "485817                        VLDB       2002   \n",
       "2581657  ACM Trans. Database Syst.       1996   \n",
       "3835435          SIGMOD Conference       1999   \n",
       "2575416              SIGMOD Record       2003   \n",
       "...                            ...        ...   \n",
       "5959276          SIGMOD Conference       1996   \n",
       "5809458                       VLDB       2000   \n",
       "250584           SIGMOD Conference       1998   \n",
       "4889105              SIGMOD Record       1998   \n",
       "5441258  ACM Trans. Database Syst.       2001   \n",
       "\n",
       "                                               right_title  \\\n",
       "id                                                           \n",
       "4003186  Olympic records for data at the 1998 Nagano games   \n",
       "485817   Automatic subspace clustering of high dimensio...   \n",
       "2581657                    Distinguished database profiles   \n",
       "3835435    Very Large Databases: How Large, How Different?   \n",
       "2575416  Data extraction and transformation for the dat...   \n",
       "...                                                    ...   \n",
       "5959276  Lore: a database management system for semistr...   \n",
       "5809458  Reordering Query Execution in Tertiary Memory ...   \n",
       "250584   Cost models for overlapping and multiversion s...   \n",
       "4889105   Index configuration in object-oriented databases   \n",
       "5441258                                    Guest editorial   \n",
       "\n",
       "                                             right_authors  \\\n",
       "id                                                           \n",
       "4003186                                 Edwin R. Lassettre   \n",
       "485817   Rakesh Agrawal, Johannes Gehrke, Dimitrios Gun...   \n",
       "2581657                                  Marianne Winslett   \n",
       "3835435                                   David Vaskevitch   \n",
       "2575416                                        Case Squire   \n",
       "...                                                    ...   \n",
       "5959276  Jason McHugh, Serge Abiteboul, Roy Goldman, Da...   \n",
       "5809458               Sunita Sarawagi, Michael Stonebraker   \n",
       "250584             Yufei Tao, Dimitris Papadias, Jun Zhang   \n",
       "4889105                                      Elisa Bertino   \n",
       "5441258           Vijay Atluri, Anupam Joshi, Yelena Yesha   \n",
       "\n",
       "                                               right_venue  right_year  \n",
       "id                                                                      \n",
       "4003186     International Conference on Management of Data        1998  \n",
       "485817      International Conference on Management of Data        1998  \n",
       "2581657                                 ACM SIGMOD Record         2002  \n",
       "3835435                              Very Large Data Bases        1995  \n",
       "2575416     International Conference on Management of Data        1995  \n",
       "...                                                    ...         ...  \n",
       "5959276                                 ACM SIGMOD Record         1997  \n",
       "5809458                              Very Large Data Bases        1996  \n",
       "250584        ACM Transactions on Database Systems (TODS)         2002  \n",
       "4889105  The VLDB Journal &mdash; The International Jou...        1994  \n",
       "5441258  The VLDB Journal &mdash; The International Jou...        2003  \n",
       "\n",
       "[162894 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_df_renamed =left_df.reset_index().add_prefix('left_')\n",
    "left_df_renamed['join'] = 1\n",
    "right_df_renamed =right_df.reset_index().add_prefix('right_')\n",
    "right_df_renamed['join'] = 1\n",
    "final_df_full = pd.merge(left_df_renamed, right_df_renamed, on=\"join\")\n",
    "final_df_full.insert(0,'label',0)\n",
    "final_df_full['combined_index'] = list(zip(final_df_full.left_id, final_df_full.right_id))\n",
    "final_df_full.loc[final_df_full['combined_index'].isin(matches_tuples),'label'] = 1\n",
    "final_df_blocked = final_df_full.loc[final_df_full['combined_index'].isin(candidate_links)]\n",
    "final_df = final_df_blocked.drop(columns=['combined_index','left_id','right_id','join'])\n",
    "final_df.index.name = 'id'\n",
    "final_df = final_df.sample(frac=1)\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train/validation/test sets\n",
    "Now that our dataset is the right format, we create a train, validation and test set by applying a 60:20:20 split. The resulting datasets are saved to csv-files inside the Data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split,validate_split,test_split = np.split(final_df, [int(.6 * len(final_df)), int(.8 * len(final_df))])\n",
    "train_file = 'Data/train_block.csv'\n",
    "validate_file = 'Data/validate_block.csv'\n",
    "test_file = 'Data/test_block.csv'\n",
    "train_split.to_csv(train_file)\n",
    "validate_split.to_csv(validate_file)\n",
    "test_split.to_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and feature engineering\n",
    "We now let [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) apply its standard data preprecessing and feature engineering procedure. Due to the time limits of the Project Challenge, we have not delved deeper into this, but rather used it as a black box. To get a rough idea of what [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) does behind the scenes, we display the still human-interpretable raw data table, which holds the result of text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading and processing data from \".\\Data/train_block.csv\"\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \".\\Data/validate_block.csv\"\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \".\\Data/test_block.csv\"\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Building vocabulary\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:06\n",
      "\n",
      "Computing principal components\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:55\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>left_title</th>\n",
       "      <th>left_authors</th>\n",
       "      <th>left_venue</th>\n",
       "      <th>left_year</th>\n",
       "      <th>right_title</th>\n",
       "      <th>right_authors</th>\n",
       "      <th>right_venue</th>\n",
       "      <th>right_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003186</td>\n",
       "      <td>0</td>\n",
       "      <td>semistructured und structured data in the web ...</td>\n",
       "      <td>paolo merialdo , giansalvatore mecca , paolo a...</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>1997</td>\n",
       "      <td>olympic records for data at the 1998 nagano games</td>\n",
       "      <td>edwin r. lassettre</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>485817</td>\n",
       "      <td>0</td>\n",
       "      <td>an automated system for web portal personaliza...</td>\n",
       "      <td>philip s. yu , charu c. aggarwal</td>\n",
       "      <td>vldb</td>\n",
       "      <td>2002</td>\n",
       "      <td>automatic subspace clustering of high dimensio...</td>\n",
       "      <td>rakesh agrawal , johannes gehrke , dimitrios g...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2581657</td>\n",
       "      <td>0</td>\n",
       "      <td>declustering of key-based partitioned signatur...</td>\n",
       "      <td>pavel zezula , paolo ciaccia , paolo tiberio</td>\n",
       "      <td>acm trans . database syst .</td>\n",
       "      <td>1996</td>\n",
       "      <td>distinguished database profiles</td>\n",
       "      <td>marianne winslett</td>\n",
       "      <td>acm sigmod record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3835435</td>\n",
       "      <td>0</td>\n",
       "      <td>the jungle database search engine</td>\n",
       "      <td>linas bukauskas , curtis e. dyreson , michael ...</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1999</td>\n",
       "      <td>very large databases : how large , how differe...</td>\n",
       "      <td>david vaskevitch</td>\n",
       "      <td>very large data bases</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2575416</td>\n",
       "      <td>0</td>\n",
       "      <td>information integration on the web : a view fr...</td>\n",
       "      <td>subbarao kambhampati , craig a. knoblock</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>2003</td>\n",
       "      <td>data extraction and transformation for the dat...</td>\n",
       "      <td>case squire</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97731</th>\n",
       "      <td>4554620</td>\n",
       "      <td>0</td>\n",
       "      <td>static detection of security flaws in object-o...</td>\n",
       "      <td>keishi tajima</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1996</td>\n",
       "      <td>integrating reliable memory in databases</td>\n",
       "      <td>wee teck ng , peter m. chen</td>\n",
       "      <td>very large data bases</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97732</th>\n",
       "      <td>4959874</td>\n",
       "      <td>0</td>\n",
       "      <td>foundations of preferences in database systems</td>\n",
       "      <td>werner kieÃŸling</td>\n",
       "      <td>vldb</td>\n",
       "      <td>2002</td>\n",
       "      <td>metu interoperable database system</td>\n",
       "      <td>asuman dogac , ugur halici , ebru kilic , gokh...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97733</th>\n",
       "      <td>3486577</td>\n",
       "      <td>0</td>\n",
       "      <td>a data warehousing architecture for enabling s...</td>\n",
       "      <td>yannis kotidis</td>\n",
       "      <td>vldb</td>\n",
       "      <td>2001</td>\n",
       "      <td>query execution techniques for caching expensi...</td>\n",
       "      <td>joseph m. hellerstein , jeffrey f. naughton</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97734</th>\n",
       "      <td>1168443</td>\n",
       "      <td>0</td>\n",
       "      <td>aqr-toolkit : an adaptive query routing middle...</td>\n",
       "      <td>henrique paques , wei tang , calton pu , wei h...</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>2000</td>\n",
       "      <td>an adaptive hybrid server architecture for cli...</td>\n",
       "      <td>kaladhar voruganti , m. tamer &amp; # 214 ; zsu , ...</td>\n",
       "      <td>very large data bases</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97735</th>\n",
       "      <td>5606068</td>\n",
       "      <td>0</td>\n",
       "      <td>efficiently mining long patterns from databases</td>\n",
       "      <td>roberto j. bayardo jr .</td>\n",
       "      <td>sigmod conference</td>\n",
       "      <td>1998</td>\n",
       "      <td>eliminating costly redundant computations from...</td>\n",
       "      <td>fran &amp; # 231 ; ois llirbat , fran &amp; # 231 ; oi...</td>\n",
       "      <td>international conference on management of data</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97736 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label                                         left_title  \\\n",
       "0      4003186      0  semistructured und structured data in the web ...   \n",
       "1       485817      0  an automated system for web portal personaliza...   \n",
       "2      2581657      0  declustering of key-based partitioned signatur...   \n",
       "3      3835435      0                  the jungle database search engine   \n",
       "4      2575416      0  information integration on the web : a view fr...   \n",
       "...        ...    ...                                                ...   \n",
       "97731  4554620      0  static detection of security flaws in object-o...   \n",
       "97732  4959874      0     foundations of preferences in database systems   \n",
       "97733  3486577      0  a data warehousing architecture for enabling s...   \n",
       "97734  1168443      0  aqr-toolkit : an adaptive query routing middle...   \n",
       "97735  5606068      0    efficiently mining long patterns from databases   \n",
       "\n",
       "                                            left_authors  \\\n",
       "0      paolo merialdo , giansalvatore mecca , paolo a...   \n",
       "1                       philip s. yu , charu c. aggarwal   \n",
       "2           pavel zezula , paolo ciaccia , paolo tiberio   \n",
       "3      linas bukauskas , curtis e. dyreson , michael ...   \n",
       "4               subbarao kambhampati , craig a. knoblock   \n",
       "...                                                  ...   \n",
       "97731                                      keishi tajima   \n",
       "97732                                    werner kieÃŸling   \n",
       "97733                                     yannis kotidis   \n",
       "97734  henrique paques , wei tang , calton pu , wei h...   \n",
       "97735                            roberto j. bayardo jr .   \n",
       "\n",
       "                        left_venue left_year  \\\n",
       "0                    sigmod record      1997   \n",
       "1                             vldb      2002   \n",
       "2      acm trans . database syst .      1996   \n",
       "3                sigmod conference      1999   \n",
       "4                    sigmod record      2003   \n",
       "...                            ...       ...   \n",
       "97731            sigmod conference      1996   \n",
       "97732                         vldb      2002   \n",
       "97733                         vldb      2001   \n",
       "97734            sigmod conference      2000   \n",
       "97735            sigmod conference      1998   \n",
       "\n",
       "                                             right_title  \\\n",
       "0      olympic records for data at the 1998 nagano games   \n",
       "1      automatic subspace clustering of high dimensio...   \n",
       "2                        distinguished database profiles   \n",
       "3      very large databases : how large , how differe...   \n",
       "4      data extraction and transformation for the dat...   \n",
       "...                                                  ...   \n",
       "97731           integrating reliable memory in databases   \n",
       "97732                 metu interoperable database system   \n",
       "97733  query execution techniques for caching expensi...   \n",
       "97734  an adaptive hybrid server architecture for cli...   \n",
       "97735  eliminating costly redundant computations from...   \n",
       "\n",
       "                                           right_authors  \\\n",
       "0                                     edwin r. lassettre   \n",
       "1      rakesh agrawal , johannes gehrke , dimitrios g...   \n",
       "2                                      marianne winslett   \n",
       "3                                       david vaskevitch   \n",
       "4                                            case squire   \n",
       "...                                                  ...   \n",
       "97731                        wee teck ng , peter m. chen   \n",
       "97732  asuman dogac , ugur halici , ebru kilic , gokh...   \n",
       "97733        joseph m. hellerstein , jeffrey f. naughton   \n",
       "97734  kaladhar voruganti , m. tamer & # 214 ; zsu , ...   \n",
       "97735  fran & # 231 ; ois llirbat , fran & # 231 ; oi...   \n",
       "\n",
       "                                          right_venue right_year  \n",
       "0      international conference on management of data       1998  \n",
       "1      international conference on management of data       1998  \n",
       "2                                   acm sigmod record       2002  \n",
       "3                               very large data bases       1995  \n",
       "4      international conference on management of data       1995  \n",
       "...                                               ...        ...  \n",
       "97731                           very large data bases       1997  \n",
       "97732  international conference on management of data       1996  \n",
       "97733  international conference on management of data       1996  \n",
       "97734                           very large data bases       1999  \n",
       "97735  international conference on management of data       1997  \n",
       "\n",
       "[97736 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, validate, test = dm.data.process(\n",
    "    path='.',\n",
    "    left_prefix='left_',\n",
    "    right_prefix='right_',\n",
    "    label_attr='label',\n",
    "    id_attr='id',\n",
    "    cache=None,\n",
    "    train=train_file,\n",
    "    validation=validate_file,\n",
    "    test=test_file)\n",
    "train_table = train.get_raw_table()\n",
    "display(train_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepMatcher Model Training\n",
    "We are now ready to train our DeepMatcher model. We take over the values of he DeepMatcher [\"Getting started\"-notebook](https://github.com/anhaidgroup/deepmatcher/blob/master/examples/getting_started.ipynb), but make a couple of small adjustments. Knowing that an epoch will take about half an hour on Google Colab and 2 hours on our local machine, we limit the number of epochs to 8 instead of 10, and since we have not seen any substantial benefit of weighting in smaller experiments, we practically disable it by setting pos_neg_ratio = 1.\n",
    "\n",
    "The training function prints the performance metric F1-measure, precision and recall for both the training and the validation set at each epoch and uses the F1-measure on the validation set to select the best model. The F1-measure is more suitable than accuracy in thie case, because even in the blocked dataset, the fraction of true matches is very small (about 1.4% )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 9210006\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python-3.6.3\\lib\\site-packages\\torch\\nn\\functional.py:1946: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:12:35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time: 3032.0 | Load Time: 1326.8 || F1:  92.83 | Prec:  95.97 | Rec:  89.90 || Ex/s:  22.42\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:14:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:  385.9 | Load Time:  459.1 || F1:  97.75 | Prec:  98.86 | Rec:  96.66 || Ex/s:  38.55\n",
      "\n",
      "* Best F1: tensor(97.7477, device='cuda:0')\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:12:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time: 3058.4 | Load Time: 1323.8 || F1:  97.91 | Prec:  98.14 | Rec:  97.69 || Ex/s:  22.30\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:  370.8 | Load Time:  440.0 || F1:  98.20 | Prec:  99.09 | Rec:  97.33 || Ex/s:  40.18\n",
      "\n",
      "* Best F1: tensor(98.2022, device='cuda:0')\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:12:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time: 3038.1 | Load Time: 1308.1 || F1:  97.49 | Prec:  97.60 | Rec:  97.38 || Ex/s:  22.49\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:12:58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:  355.1 | Load Time:  424.1 || F1:  96.80 | Prec:  99.53 | Rec:  94.21 || Ex/s:  41.81\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:11:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time: 3024.1 | Load Time: 1293.9 || F1:  98.69 | Prec:  98.76 | Rec:  98.61 || Ex/s:  22.63\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:  356.6 | Load Time:  426.2 || F1:  97.51 | Prec:  99.08 | Rec:  95.99 || Ex/s:  41.61\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:11:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time: 3008.8 | Load Time: 1299.5 || F1:  99.03 | Prec:  99.23 | Rec:  98.84 || Ex/s:  22.69\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:  373.9 | Load Time:  443.5 || F1:  98.43 | Prec:  98.88 | Rec:  98.00 || Ex/s:  39.85\n",
      "\n",
      "* Best F1: tensor(98.4340, device='cuda:0')\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:14:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time: 3102.4 | Load Time: 1344.8 || F1:  99.34 | Prec:  99.38 | Rec:  99.31 || Ex/s:  21.98\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:  360.8 | Load Time:  431.4 || F1:  97.79 | Prec:  97.14 | Rec:  98.44 || Ex/s:  41.12\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:12:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time: 3050.1 | Load Time: 1315.4 || F1:  99.58 | Prec:  99.61 | Rec:  99.54 || Ex/s:  22.39\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:  367.4 | Load Time:  438.4 || F1:  97.26 | Prec:  95.89 | Rec:  98.66 || Ex/s:  40.43\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 01:12:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time: 3014.6 | Load Time: 1316.6 || F1:  99.69 | Prec:  99.85 | Rec:  99.54 || Ex/s:  22.57\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:  375.5 | Load Time:  446.3 || F1:  98.22 | Prec:  98.22 | Rec:  98.22 || Ex/s:  39.64\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(98.4340, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_summarizer='hybrid')\n",
    "model.run_train(\n",
    "    train,\n",
    "    validate,\n",
    "    epochs=8,\n",
    "    batch_size=16,\n",
    "    best_save_path='Results/model_dblp_acm_block.pth',\n",
    "    pos_neg_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepMatcher Testing\n",
    "We are now curious to see what the selected model does on the test set. We see that compared to the training and validation results, there is no serious drop in the F1-value, so we are reasonably confident that our model is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:  209.1 | Load Time:  506.0 || F1:  97.98 | Prec:  97.88 | Rec:  98.09 || Ex/s:  45.56\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(97.9809, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.run_eval(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PRLT Feature Engineering\n",
    "We are now ready to switch to the the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/), which uses a simpler approach to feature engineering. Basically, we fill a vector with zeros of ones that indicate whether a record pair matches with respect to a certain column. So for each of the columns in the left/right tables, we define how similar they need to be to be matching. All the values of the per-column-matches are then concatenated to a global feature vector of zeros and ones, where  an all zero vector means that the pair matches in none of the columns, an an all-one-vectir indicates a match with respect to every column. This vector of zeros and ones is then fed to the various classifier models.\n",
    "\n",
    "To define when we call two colum values matching, we let ourselves be inspired by the [example](https://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html) that the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) provides in its introductory text. Here most of the column matches are defined in terms of a similarity that is based on the [Jaro Winkler distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance). The only small adaptation that we make is in the choice of the similarity threshold value for \"venue\", because a short inspection of the data indicates that matching records tend to agree less with respect to \"venue\" than they do with respect to \"title\" or 'author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column-based matching criteria that define the feature vector\n",
    "compare_cl = recordlinkage.Compare()\n",
    "compare_cl.string('title', 'title', method='jarowinkler', threshold=0.85,label = 'title')\n",
    "compare_cl.string('authors', 'authors', method='jarowinkler', threshold=0.85,label = 'authors')\n",
    "compare_cl.exact('year', 'year', label='year')\n",
    "compare_cl.string('venue', 'venue', method='jarowinkler', threshold=0.65,label = 'authors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRLT Data Loading and Feature Computation\n",
    "We now load our previously saved train/test/validation sets from file. Ironically, to feed the data to the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) methods, we need to split up the joined table into two separate tables and provide a list of links indicating which record combinations need to be considered in the computation of the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "true_links = {}\n",
    "features = {}\n",
    "validation_dict = {}\n",
    "datasets= {'train':train_file, \n",
    "'test':test_file,\n",
    "'validation':validate_file}\n",
    "\n",
    "for key in datasets:\n",
    "    df = pd.read_csv(datasets[key])\n",
    "    nof_cols = int((df.shape[1] - 2)/2)\n",
    "    dfA = df.iloc[:,2:nof_cols + 2] # left table\n",
    "    dfB = df.iloc[:,nof_cols + 2:df.shape[1]] # right table\n",
    "    # Delete 'left_' and 'right_' prefixes from column names\n",
    "    dfA.rename(columns={c:c[5:] for c in dfA.columns },inplace=True) \n",
    "    dfB.rename(columns={c:c[6:] for c in dfB.columns },inplace=True)\n",
    "\n",
    "    tuples = [(i,i) for i in range(len(df)) if df.iloc[i]['label'] == 1]\n",
    "    true_links[key] = pd.MultiIndex.from_tuples(tuples)\n",
    "\n",
    "    tuples_full = [(i,i) for i in range(len(df))]\n",
    "    candidate_links = pd.MultiIndex.from_tuples(tuples_full)\n",
    "    # Final features (used in all methods)\n",
    "    features[key] = compare_cl.compute(candidate_links, dfA, dfB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRLT Model Training\n",
    "We are now ready to train the various models included in the [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/). There are three super vised methods (Logistic regression, Naive Bayes and Support vector machines) and two unsupervised methods (K-Means and [ECM](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.3828&rep=rep1&type=pdf)). We also add a simple heuristic that we label as \"Hand-tuned\". This basically looks whether records match on the majority of columns (more ones than zeroes in the feature vector) and classifies a pair as a match if this is the case. We assume that this heuristic as a proxy for traditional programming methods as implemented in tools like Power BI, since these are also based on string edit distances. \n",
    "\n",
    "The results of the heuristic and the various models on the validation set is displayed below. As we can see, all the supervised  trained models clearly outperform the simple heuristic, whereas the best [Python Record Linkage Toolkit](https://recordlinkage.readthedocs.io/en/latest/) model only slightly underperforms the much more complicated and time-consuming [DeepMatcher](https://anhaidgroup.github.io/deepmatcher/html/) model. Interestingly, the unsupervised ECM model also is a significant gain compared to the simple heuristic, indicating that also without labels, a machine learning approach can be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********\n",
      "Hand-tuned\n",
      "***********\n",
      "Confusion matrix:\n",
      "[[  278   171]\n",
      " [   30 32100]]\n",
      "Accuracy: 0.9938303815341171\n",
      "Recall: 0.6191536748329621\n",
      "F-score: 0.7344782034346102\n",
      "\n",
      "\n",
      "********************\n",
      "Logistic regression\n",
      "********************\n",
      "Confusion matrix:\n",
      "[[  425    24]\n",
      " [   27 32103]]\n",
      "Accuracy: 0.9984345744191043\n",
      "Recall: 0.9465478841870824\n",
      "F-score: 0.9433962264150944\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python-3.6.3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "Naive Bayes\n",
      "************\n",
      "Confusion matrix:\n",
      "[[  434    15]\n",
      " [   28 32102]]\n",
      "Accuracy: 0.9986801313729703\n",
      "Recall: 0.9665924276169265\n",
      "F-score: 0.9527991218441273\n",
      "\n",
      "\n",
      "***********************\n",
      "Support vector machine\n",
      "***********************\n",
      "Confusion matrix:\n",
      "[[  425    24]\n",
      " [   27 32103]]\n",
      "Accuracy: 0.9984345744191043\n",
      "Recall: 0.9465478841870824\n",
      "F-score: 0.9433962264150944\n",
      "\n",
      "\n",
      "********\n",
      "K-means\n",
      "********\n",
      "Confusion matrix:\n",
      "[[  449     0]\n",
      " [ 3441 28689]]\n",
      "Accuracy: 0.8943798152183923\n",
      "Recall: 1.0\n",
      "F-score: 0.20696012906199585\n",
      "\n",
      "\n",
      "****\n",
      "ECM\n",
      "****\n",
      "Confusion matrix:\n",
      "[[  437    12]\n",
      " [   89 32041]]\n",
      "Accuracy: 0.9968998434574419\n",
      "Recall: 0.9732739420935412\n",
      "F-score: 0.8964102564102564\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers= {\n",
    "'Hand-tuned':None,\n",
    "'Logistic regression':recordlinkage.LogisticRegressionClassifier(),\n",
    "'Naive Bayes': recordlinkage.NaiveBayesClassifier(),\n",
    "'Support vector machine': recordlinkage.SVMClassifier(),\n",
    "'K-means': recordlinkage.KMeansClassifier(),\n",
    "'ECM': recordlinkage.ECMClassifier()}\n",
    "\n",
    "for key in classifiers:\n",
    "    validation_dict[key] = {}\n",
    "    if key == 'Hand-tuned':\n",
    "        # Immediate prediction\n",
    "        result = features['validation'][features['validation'].sum(axis=1) > 2].index\n",
    "    else:\n",
    "        # Training \n",
    "        if key == 'ECM' or key == 'K-means':\n",
    "            classifiers[key].fit(features['train']) \n",
    "        else:\n",
    "            classifiers[key].fit(features['train'], true_links['train'])\n",
    "        # Predict the match status for all test record pairs\n",
    "        result =  classifiers[key].predict(features['validation'])\n",
    "        \n",
    "    # Validate\n",
    "    validation_dict[key] = performance_metrics(true_links['validation'], result, len(features['validation']))\n",
    "    \n",
    "    #Print results\n",
    "    print_results(key, validation_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRLT Testing\n",
    "We select the best model based on the F1-measure on the vaildation set and give it the test set to classify. The result is displayed below. Notice that the result from DeepMatcher is only slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Selected model (Naive Bayes) on test set\n",
      "*****************************************\n",
      "Confusion matrix:\n",
      "[[  451    19]\n",
      " [   18 32091]]\n",
      "Accuracy: 0.9988642990883698\n",
      "Recall: 0.9595744680851064\n",
      "F-score: 0.9605963791267307\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_scores = {key:validation[key]['f-score'] for key in validation}\n",
    "best_model = max(f_scores, key = f_scores.get) \n",
    "if best_model == 'Hand-tuned':\n",
    "    result = features['test'][features['test'].sum(axis=1) > 2].index\n",
    "else:\n",
    "    result = classifiers[best_model].predict(features['test'])\n",
    "test_dict =  performance_metrics(true_links['test'], result, len(features['test']))\n",
    "print_results(\"Selected model ({}) on test set\".format(best_model), test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Our analysis seems to indicate that machine learning approaches can be beneficial in the context of entity matching, but that the question whether deep learning really outperforms more traditional M models can eb debated. Much will probably bedepend on the nature of the dataset and  the amount of time that one wished to invets in labeling data and training the model. An interesting observation is that at least on out not too complicated dataset, the unsupervised ECM method also show a very decent performance, which indicates that one can already gain soemthing from machine learning without labeling the data.\n",
    "\n",
    "Furyher research could be geared towards the effect of fetaure engineering on the methods (DeepMatcher used other geatrues than PRLT) and on the question to what extent our simple heuristic was a good proxy to traditional programming methods implemented in existing entity matching systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
